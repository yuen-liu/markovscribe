{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuen-liu/markovscribe/blob/main/Markov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hYvyO20ays0"
      },
      "source": [
        "# Methods of Text Completion: Markov Chains and LLMs\n",
        "by: Tejas Tirthapura (tt3021), Bridget Gwyneth Liu (bgl2126), Richard Li (rjl2194)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPffagcCblvP"
      },
      "source": [
        "# Abstract\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoroAwX5fRpN"
      },
      "source": [
        "In this study, we compare two approaches to new-word prediction in natural language. The LLM (Large Language Model) and the n-gram natural language model. First, we explore the use of Markov Chains to build an n-gram-based natural language model, which will predict the next word in a sequence based solely on the previous few words. We implement a n-gram generator, represent the learned relationships as transition matrices, and visualize them as weighted directed graphs. Our model uses a straightforward probabilistic approach grounded in the Markov property, assuming that the next state depends only on the current state, not the full history. The model learns transition probabilities over bigrams and trigrams via simple counts and smoothing, producing an interpretable probability table and weighted transition graph.\n",
        "Next, we use a basic Large Language Model (LLM) using transformers for comparison. Unlike the Markov model, the LLM can theoretically capture longer dependencies and more complex patterns in text, but requires significantly more computation, larger datasets, and careful tuning to avoid overfitting.\n",
        "\n",
        "\n",
        "Our findings show that Markov n-gram models are simple, fast to train, and effective on small or structured datasets, but struggle with ambiguity and long-term context. In contrast, LLM models offer more predictive power and flexibility but at the cost of higher computational requirements and longer training times. Ultimately, the Markov Chain approach provides a powerful yet interpretable baseline, while neural models offer improved performance when scaling to richer, more complex text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjbUj2fIcOGJ"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp3BOQEogdvJ"
      },
      "source": [
        "A Markov Chain is a mathematical system that undergoes transitions from one state to another within a finite or countable set of states (1). Markov Chains are unique because they must satisfy the Markov Property:\n",
        "\n",
        "The future state of the system depends only on the present state, and not on the sequence of events that preceded it.\n",
        "\n",
        "In other words, the system has no memory of the past beyond its current state. This \"memorylessness\" makes Markov Chains particularly useful for modeling processes where the next outcome can be predicted based purely on the present.\n",
        "\n",
        "In the context of Natural Language Processing (NLP), a Markov Chain can be used to predict the next word in a sentence by considering only the previous word (for a bigram model) or the previous few words (for higher-order n-grams).\n",
        "We represent these probabilities using a transition matrix or graph, where each node is a word or phrase, and each directed edge represents the probability of moving from one word to the next.\n",
        "\n",
        "Thus, using Markov Chains establishes the basis of many classic language models, allowing systems to generate, autocomplete, or even correct human language input in a probabilistic, interpretable way. Today, speech recognition, handwriting recognition, information retrieval, data compression, and even spam filtering all utilize this technique (2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRXRU4M4gocX"
      },
      "source": [
        "In our project, we model word prediction as a Markov Chain by treating each word (or sequence of words) as a state.\n",
        "The idea is that as we move through a piece of text word by word, we are essentially transitioning from one state (word) to the next.\n",
        "\n",
        "For example, in a bigram model (n=2), the probability of seeing the next word depends only on the current word ‚Äî not the entire sentence history ‚Äî which exactly matches the Markov Property, or that the future state of the system depends only the present state.\n",
        "\n",
        "Similarly, in a trigram model (n=3), the current state is the last two words, and the next word depends only on that pair, not anything before them.\n",
        "\n",
        "Thus, we can represent our model as:\n",
        "\n",
        "ùëÉ ( next¬†word ‚à£ previous¬†word(s) ) = ùëÉ ( next¬†state ‚à£ current¬†state )\n",
        "\n",
        "Transitions between states are determined by how often a particular word follows a given previous word (or words) in the training text.\n",
        "We can store these transition probabilities in a transition matrix or transition graph, where the weights on the edges represent the likelihood of moving from one word (state) to the next.\n",
        "\n",
        "To be more precise, we calculated words' relative frequencies to attain these transition probabilities. This is also known as a way to find the maximum likelihood estimation or MLE of a word (3). By getting the counts of each word from the sampled text, and then dividing by some total count so that the resulting probabilities fell between 0 and 1 and summed to 1, the MLE was calculated. In the next section, the precise methodology for this is described.\n",
        "\n",
        "By learning these transitions from large text datasets, our model can then predict likely next words given a prompt ‚Äî just like a classic Markov Chain moves between states based on probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MckDKoIVgx_Y"
      },
      "source": [
        "# Markov Chain Implementation\n",
        "There are 3 main parts to implementing the markov chain code for predicting the word on the previous n words.\n",
        "\n",
        "1.   Gather the training data\n",
        "2. create the transition matrix based on the training data\n",
        "3. retrieve the appropriate data from the transition matrix to predict the next word\n",
        "\n",
        "Most of this is simple enough, but a interesting challenge is determining how to store the data in an transition matrix. We decided to store this data in a dictionary, where the key is a tuple of words that represents the state, and the value is another dictionary in which the key is the next word, and the value is the count that it appears in the training data.\n",
        "\n",
        "Lets dive into how this code is actually implemented now."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gathering the training data\n",
        "\n",
        "We chose to use books from the [Gutenberg Project](https://www.gutenberg.org/) because they provide easily accessible text files of full ebooks with a large number of words. Each book is identifiable by an ID number, so we implemented a function that returns the string of a book given its specific ID. In this function, we retrieve the text content of the response, remove the header and footer, and replace all characters that are not letters, apostrophes, or spaces with a single space.\n",
        "\n",
        "We then created another function that gathers the strings of the ten most popular books on the Gutenberg Project and returns a single combined string. These books are:\n",
        "* *Frankenstein* by Mary Wollstonecraft Shelley\n",
        "* *Moby Dick* by Herman Melville\n",
        "* *Pride and Prejudice* by Jane Austen\n",
        "* *Romeo and Juliet* by William Shakespeare\n",
        "* *Alice's Adventures in Wonderland* by Lewis Carroll\n",
        "* *A Doll's House* by Henrik Ibsen\n",
        "* *The Great Gatsby* by F. Scott Fitzgerald\n",
        "* *The Importance of Being Earnest* by Oscar Wilde\n",
        "* The Complete Works of William Shakespeare\n",
        "* *Middlemarch* by George Eliot\n",
        "\n"
      ],
      "metadata": {
        "id": "c3SWZXlQY7rj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neWn0wW1vxBR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_book(book_number):\n",
        "    \"\"\"\n",
        "    Downloads and returns the text of a Gutenberg book by its number.\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"https://www.gutenberg.org/cache/epub/{book_number}/pg{book_number}.txt\")\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch plain text file, status code {response.status_code}\")\n",
        "\n",
        "    text = response.text\n",
        "\n",
        "    # Clean header and footer\n",
        "    start_marker = \" ***\"\n",
        "    end_marker = \"*** END OF\"\n",
        "\n",
        "    start_idx = text.find(start_marker)\n",
        "    end_idx = text.find(end_marker)\n",
        "\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        book_text = text[start_idx:end_idx]\n",
        "    else:\n",
        "        book_text = text\n",
        "    cleaned_text = \"\"\n",
        "    for c in book_text:\n",
        "      if c.isalpha() or c.isspace() or c == \"'\":\n",
        "        cleaned_text += c\n",
        "      else:\n",
        "        cleaned_text += \" \"\n",
        "\n",
        "    return cleaned_text.lower().strip()\n",
        "\n",
        "def generate_all_books():\n",
        "  final_string = \"\"\n",
        "  for i in {84, 2701, 1342, 11, 1513, 64317, 2542, 844, 26184, 100}: #, 145, 2641, 174, 37106, 16389, 67979, 345, 2554, 394, 6761, 43, 6593, 2160, 4085, 5200\n",
        "    final_string += get_book(i)\n",
        "  return final_string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Markov Chain\n",
        "\n",
        "As stated before, storing the Markov chain is a complex task. Let's once again unpack the data we need to store for it to function. The Markov chain has states, which we represent as tuples of words, and each state can transition to many different words with certain probabilities. To implement this, we turn the input string into a dictionary where the keys are tuples of words and the values are themselves dictionaries that map the next word to a count of how often it appears. We use counts instead of probabilities because probabilities would require iterating through all transitions for each state and dividing by the total count‚Äîthis would add extra code and reduce readability. While using a dictionary works for our purposes, it‚Äôs not the most efficient structure for lookup‚Äîfinding the most frequent word requires scanning every key in the dictionary. A more optimal structure like a red-black tree could offer faster lookups, though this may be unnecessary for our current scope.\n",
        "\n",
        "We implement the Markov chain such that it uses the previous n-1 words to predict the next word, rather than a fixed number. We define a class to represent the Markov chain, storing n in self.n and the dictionary in self.ngram_dict.\n",
        "\n",
        "To construct this large dictionary in Python, we first split the input string into tokens. Then, for each sequence of tokens, we call the learn function. If the tuple key already exists in the dictionary, we increment the count of the following word; otherwise, we create a new entry and set its count to 1.\n",
        "\n",
        "This leads to a slight problem when predicting the first n-1 words. If we don't have entries in the dictionary for shorter sequences, how do we generate those initial words? To solve this, we add to the dictionary not only tuples of length n-1, but also of every length from 1 to n-1. This additional data also helps when we can‚Äôt find the current state in the dictionary‚Äîwe can fall back to a shorter key by trying n-2 words, then n-3, and so on, until we get results. If we still can't find a match, we resort to guessing.\n",
        "\n",
        "Finally, we create a function called babble that takes an initial string and returns the next m words."
      ],
      "metadata": {
        "id": "BqhipkoNeb7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XsGX_pFmR3D"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class MarkovChain:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngram_dict = {}  # {tuple: {next_word: count}}\n",
        "\n",
        "    def _learn_key(self, key, value):\n",
        "        if key in self.ngram_dict:\n",
        "            if value in self.ngram_dict[key]:\n",
        "              self.ngram_dict[key][value] += 1\n",
        "            else:\n",
        "              self.ngram_dict[key][value] = 1\n",
        "        else:\n",
        "            self.ngram_dict[key] = {value: 1}\n",
        "\n",
        "    def learn(self, text):\n",
        "        words = text.split()\n",
        "        # Create overlapping n-grams\n",
        "        for k in range(1, self.n + 1):\n",
        "          for i in range(len(words) - k + 1):\n",
        "              ngram = words[i:i + k]\n",
        "              key = tuple(ngram[:-1])  # n-1 words as key\n",
        "              value = ngram[-1]         # nth word as value\n",
        "              self._learn_key(key, value)\n",
        "\n",
        "    def _next_word(self, current_state):\n",
        "        # Convert current_state (string) to a tuple of words\n",
        "        state_words = current_state.split()\n",
        "        key = tuple(state_words[-(self.n - 1):])\n",
        "        len_offset = 1;\n",
        "        while (key not in self.ngram_dict):\n",
        "          key = tuple(state_words[-(self.n - 2 - len_offset):])\n",
        "          len_offset += 1\n",
        "          if (len_offset > self.n - 2):\n",
        "            # Fallback: pick a random key's most common word\n",
        "            random_key = random.choice(list(self.ngram_dict.keys()))\n",
        "            return self._next_word(\" \".join(random_key))  # Recurse with valid key\n",
        "\n",
        "        nw = \"\"\n",
        "        nw_frequency = -1\n",
        "\n",
        "        for k in self.ngram_dict[key]:\n",
        "            if self.ngram_dict[key][k] > nw_frequency:\n",
        "              nw = k\n",
        "              nw_frequency = self.ngram_dict[key][k]\n",
        "        return nw\n",
        "\n",
        "    def babble(self, amount, state=\"\"):\n",
        "        if amount <= 0:\n",
        "            return state.strip()\n",
        "        next_word = self._next_word(state)\n",
        "        if not next_word:\n",
        "            return state.strip()\n",
        "        new_state = f\"{state} {next_word}\".strip()\n",
        "        return self.babble(amount - 1, new_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainingdata = generate_all_books()"
      ],
      "metadata": {
        "id": "SzNM4gsop7sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_Vo6_4ojThZ"
      },
      "outputs": [],
      "source": [
        "mc2 = MarkovChain(2)\n",
        "mc2.learn(trainingdata)\n",
        "mc3 = MarkovChain(3)\n",
        "mc3.learn(trainingdata)\n",
        "mc4 = MarkovChain(4)\n",
        "mc4.learn(trainingdata)\n",
        "mc6 = MarkovChain(6)\n",
        "mc6.learn(trainingdata)\n",
        "mc8 = MarkovChain(8)\n",
        "mc8.learn(trainingdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTBaKHP-jb3L",
        "outputId": "4a5f4fc6-5458-4bc4-fd7a-80394d489d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mc2: i am i am i am i am i am i am i am i am i am i am i\n",
            "Mc3: i am not in the world s a good deal of good hope and fear not my lord i ll be\n",
            "Mc4: i am not in the least of all these piteous woes we cannot without circumstance descry re enter some of the\n",
            "Mc6: i am not in the mind but i were better to be married of him than of another for he is\n",
            "Mc8: i am not in the mind but i were better to be married of him than of another for he is\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Mc2: {mc2.babble(20, 'i')}\")\n",
        "print(f\"Mc3: {mc3.babble(20, 'i')}\")\n",
        "print(f\"Mc4: {mc4.babble(20, 'i')}\")\n",
        "print(f\"Mc6: {mc6.babble(20, 'i')}\")\n",
        "print(f\"Mc8: {mc8.babble(20, 'i')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Observations:\n",
        "In the bigram (Mc2) model the text tends to be repetitive with very little context. This is because in a bigram model the next word is only based on the previous word in the sentance and the most likely word to follow I is am and vice versa, creating an infinite loop.\n",
        "\n",
        "In a mid-sized n-gram model like Mc3 or Mc4, there is less repetition and more variety. This is because there is adequate data to create the markov chain where the model can provide more than one result for the given state and not get stuck in a loop.\n",
        "\n",
        "For longer n-grams (Mc6, Mc8), as the n-gram size increases, the output becomes more contextually relevant, with longer phrases and more coherent text. However, for this specific input data, the models spit out the exact same text from Shakespeare's \"As You Like It\". This is becuase the input data is not big enough for the model to have more than one result given such a long string of words before it. The longer n-gram models provide more coherent text but still rely heavily on common phrases found in the training data. The improvements in text quality are clear but limited by the data."
      ],
      "metadata": {
        "id": "OjBglQfNwGtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Implementation\n",
        "\n",
        "A Large Language Model (LLM) is a type of deep learning model trained on vast amounts of text data to understand, generate, and manipulate human language. Built on the transformer architecture, LLMs capture complex relationships between words in a given context. During pretraining, the model learns to predict the next word or token in a sequence, which enables it to generate coherent and contextually appropriate text. Our implementation of an LLM involves loading a pretrained model and tokenizer from Hugging Face library. The input text is first tokenized into numerical representations, then passed into the model, which generates output tokens based on learned patterns. These tokens are decoded back into text, producing results such as completions like what the markov chain does."
      ],
      "metadata": {
        "id": "N4xkorRaszJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WkbN3cuSvtjv",
        "outputId": "99fd2520-745c-45c6-8d15-236ef90657f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "class LLMBabbler:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "        self.model.eval()\n",
        "        self.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _get_next_word(self, prompt):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1,\n",
        "                do_sample=False\n",
        "            )\n",
        "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return decoded[len(prompt):].strip()\n",
        "\n",
        "    def babble(self, start_text, num_words):\n",
        "        text = start_text.strip()\n",
        "        for _ in range(num_words):\n",
        "            next_word = self._get_next_word(text)\n",
        "            if not next_word:\n",
        "                break\n",
        "            text += \" \" + next_word\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "dNxQhEDputJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLMBabbler()\n",
        "print(llm.babble(\"i am\", 20))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwK2lMF4uzCh",
        "outputId": "e404be19-f5b2-4c1b-ded9-99c926091825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am i am a bit of a fan of the game , but I 'm not sure if I 'm a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Comparison between Markov Chain and LLM\n",
        "\n",
        "The LLM clearly has more creativity and variety in its response, but when starting with such a small start text, it is limited in what it can come up with. A LLM likely would serve much better for longer start texts and tasks like answering questions, while the markov chain serves best for quick completions based on a short context. The LLM also takes a much longer time to train. It doesn't take long in our code because we use a pretrained LLM, while we gather our own data and train our markov chain with it.\n"
      ],
      "metadata": {
        "id": "mH3LT1GjxqYS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9HtlVDpcvK9"
      },
      "source": [
        "# Further Implications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INY4CZFolaqw"
      },
      "source": [
        "Building an n-gram language model using Markov Chains opens up several broader possibilities and connections.\n",
        "\n",
        "Understanding Language Generation: Simple Markov models show how coherent-seeming text can be generated purely based on local word patterns without needing deep understanding, highlighting the structure embedded even in casual human writing. For example, bigrams and trigrams can be reasonably accurate in reproducing plausible word sequences (in the, out of, etc). This extends to larger word sequences. Even ‚Äúmemoryless‚Äù models can retain the human-aspect of text through the probabilities of specific words following other words.\n",
        "\n",
        "Baseline for More Complex Models: Markov-based models serve as a strong, interpretable baseline for language modeling, against which more complex models like RNNs and Transformers can be evaluated. N-grams models are beneficial because they are so simple. This means that, in the future, more complex models can be compared to the simple ‚Äúvanilla‚Äù model. If a more ‚Äúcomplex‚Äù model underperforms compared to the basic n-gram model, then it‚Äôs an indicator that the architecture has either a fundamental problem, or is too complex for its purpose.\n",
        "\n",
        "Applications to Autocomplete and Spell Checking: Predicting the next word based on previous context is the foundation for many real-world applications, such as phone keyboards, search engine suggestions, and smart email composition. More importantly, n-gram lookups are incredibly efficient, which is beneficial for devices with tight latency and limited memory. This can be important for potentially doing word prediction, like in iphone keyboard functions, or in search-engine autocomplete.\n",
        "\n",
        "Insights into Data Sparsity and Smoothing: Higher-order n-gram models have challenges with data sparsity; 3-word and 4-word probability sequences are unlikely and almost impossible respectively. When uncorrected, random backoffs are inevitable, as the model tries to assign probabilities to these events. Future developments into solutions like smoothing, backoff models, and more general machine learning techniques will happen to create methods that deal with these rare events.\n",
        "\n",
        "Connections to Modern Deep Learning: Although simple, n-gram models using Markov Chains reflect early versions of what modern deep learning models attempt to generalize: learning transition patterns between tokens in a sequence (which is now done in much more powerful, distributed ways). The core of the algorithm still exists. RNNs and Transformers, by allowing for recurrence or self-attention, side-step the downsides of n-grams and are a future connection.\n",
        "\n",
        "Extensions to Other Fields: Markov Chains aren't limited to text. This type of modeling appears in genetics (predicting DNA sequences), economics (predicting market states), game AI (predicting player moves), and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HufaatmNdDmO"
      },
      "source": [
        "# Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67on1NZsl253"
      },
      "source": [
        "Though simple, our Markov Chain-based n-gram model offers key insights into how language can be statistically modeled and predicted. By working through this project, we not only explore the foundational ideas behind modern language processing systems but also build a practical, interpretable system that highlights both the power and the limitations of memoryless models. This hands-on understanding prepares us to appreciate and engage with more advanced methods, such as Recurrent Neural Networks and Transformer-based architectures, that build on and extend these ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MFy4EcOdLjy"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. https://www.publichealth.columbia.edu/research/population-health-methods/markov-chain-monte-carlo\n",
        "2. https://www.mecs-press.org/ijitcs/ijitcs-v14-n2/IJITCS-V14-N2-1.pdf\n",
        "3. https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "4. https://arxiv.org/html/2410.02724v1\n",
        "5. https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "6. https://sookocheff.com/post/nlp/\n",
        "7. https://www.gutenberg.org/ebooks/search/?sort_order=downloads\n",
        "\n"
      ],
      "metadata": {
        "id": "_DdEyuaXN0em"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}